{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "\n",
    "An expectation of a random variable is a weighted average\n",
    "\n",
    "$E[f(X)] = \\sum_{x=1}^{\\infty}f(x)p(x) \\\\ E[f(X)]= \\int_{-\\infty}^{\\infty} f(x)p(x)dx $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expactations of contants or known values:\n",
    "\n",
    "$E[a] = a$\n",
    "\n",
    "$E[Y | Y = y] = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: What is the expectation of the roll of die\n",
    "\n",
    "f(x): the result when roll the die\n",
    "p(x): the probability of roll the die with result x\n",
    "\n",
    "f(1) = 1, p(1) = 1/6\n",
    "\n",
    "f(2) = 2, p(2) = 1/6\n",
    "\n",
    "f(3) = 3, p(3) = 1/6\n",
    "\n",
    "f(4) = 4, p(4) = 1/6\n",
    "\n",
    "f(5) = 5, p(5) = 1/6\n",
    "\n",
    "f(6) = 6, p(6) = 1/6\n",
    "\n",
    "$E[f(X)] = 1 * 1/6 + 2 * 1/6 + 3 * 1/6 + 4 * 1/6 + 5 * 1/6 + 6 * 1/6 = 3.5%$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** What is the expectation of the sum of two die\n",
    "\n",
    "f(1) = 1, p(1) = 0\n",
    "\n",
    "f(2) = 2, p(2) = p(x1 = 1, x2 = 2) = 1/6 * 1/6 = 1/36\n",
    "\n",
    "f(3) = 3, p(3) = p(x1 = 1, x2 = 2) + p(x1 = 2, x2 = 1) = 1/36 + 1/36 = 2/36\n",
    "\n",
    "f(4) = 4, p(4) = 3/36\n",
    "\n",
    "4 = 1 + 3 = 2 + 2 = 3 + 1\n",
    "\n",
    "f(5) = 5, p(5) = 4/36\n",
    "\n",
    "5 = 1 + 4 = 2 + 3 = 3 + 2 = 4 + 1\n",
    "\n",
    "f(6) = 6, p(6) = 5/36\n",
    "\n",
    "6 = 1 + 5 = 2 + 4 = 3 + 3 = 4 + 2 = 5 + 1\n",
    "\n",
    "f(7) = 7, p(7) = 6/36\n",
    "\n",
    "7 = 1 + 6 = 2 + 5 = 3 + 4 = 4 + 3 = 5 + 2 = 6 + 1\n",
    "\n",
    "f(8) = 8, p(8) = 5/36\n",
    "\n",
    "9 = 2 + 6 = 3 + 5 = 4 + 4 = 5 + 3 = 6 + 2\n",
    "\n",
    "f(9) = 9, p(9) = 4/36\n",
    "\n",
    "9 = 3 + 6 = 4 + 5 = 5 + 4 = 6 + 3\n",
    "\n",
    "f(10) = 10, p(10) = 3/36\n",
    "\n",
    "10 = 4 + 6 = 5 + 5 = 6 + 4\n",
    "\n",
    "f(11) = 11, p(11) = 2/36\n",
    "\n",
    "11 = 5 + 6 = 6 + 5\n",
    "\n",
    "f(12) = 12, p(12) = 1/36\n",
    "\n",
    "12 = 6 + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9999999999999991"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10 , 11, 12])\n",
    "p = np.array([1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1]) / 36.0\n",
    "np.dot(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Computational Linguistics I: Maximum Entropy](https://www.youtube.com/watch?v=XqgDtD_P75A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "* Measure of disorder in a system\n",
    "* In a real world, entropy in a system tends to increase\n",
    "* Can also be applied to probabilities\n",
    "  * is one (or a few outcomes) certain (low entropy)\n",
    "  * are things equiprobable (high entropy)\n",
    "* In data science\n",
    "  * we look for features that allow us to reduce entropy (decision trees)\n",
    "  * all else beging equal, we seek models that have maxinum entropy (Occam's razor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of uncertainty that is associated with the distribution of a random variable:\n",
    "\n",
    "$$H(X) = -E[lg(p(X))] = - \\sum_{x} p(x) lg(p(x)) = - \\int^{\\infty}_{-\\infty} p(x) lg(p(x)) dx$$\n",
    "\n",
    "Does not account for the values of the random variable, only the spread of the distribution\n",
    "\n",
    "* $H(X) > 0$\n",
    "* uniform distribution = highest entropy, point mass = lowest\n",
    "* suppose $P(X = 1) = p, P(X = 0) = 1 - p$ and $P(Y = 100) = p, P(Y = 0) = 1 - p$; $X$ and $Y$ have the same entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Example** What is the entropy of a roll of a die\n",
    "\n",
    "p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1/6\n",
    "\n",
    "$H(X) = -6 * 1/6 * lg(1/6) = 2.58$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5849625007211561"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "p = np.array([1, 1, 1, 1, 1, 1]) / 6.0\n",
    "H = - np.dot(p, np.log2(p))\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** What is the entropy of roll two dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2744019192887706"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10 , 11, 12])\n",
    "p = np.array([1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1]) / 36.0\n",
    "H = - np.dot(p, np.log2(p))\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You get more entropy, because you get more outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Principle (Jaynes)\n",
    "\n",
    "> All else being equal, we should prefer distributions that maximize the Entropy\n",
    "\n",
    "The reason\n",
    "\n",
    "* you shold not assume that you know more than you do\n",
    "* if you don't have evidence that one thing should occur than other thing\n",
    "\n",
    "Question\n",
    "\n",
    "* What additional contraints do we want to place on the distribution?\n",
    "* How, mathematically, do we optimize the entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraints\n",
    "\n",
    "* We are attemping to model a probability distribution p\n",
    "* By defination, our probability distribution must sum to one\n",
    "\n",
    "$$\\sum_{x} p(x) = 1$$\n",
    "\n",
    "**Feature Containts**\n",
    "\n",
    "* We observe features across many outcomes\n",
    "* We're modelling a distribution p over observations x. What is the correct model of features under this distribution?\n",
    "* The whole point of this is that we **don't** want to count outcomes (we've discussed those models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
